@misc{cancerNIH,
    title = {How Cancer Is Diagnosed},
    author = {{National Cancer Institute}},
    howpublished = {\url{https://www.cancer.gov/about-cancer/diagnosis-staging/diagnosis}},
    note = {Last accessed: 2025-02-05},
    year = {2023}
}

@misc{LSTMolah,
    title = {Understanding {LSTM} Networks},
    author = {Olah, Christopher},
    howpublished = {\url{https://colah.github.io/posts/2015-08-Understanding-LSTMs/}},
    note = {Last accessed: 2025-03-22},
    year = {2015}}

@article{ananyaNLG,
  author       = {Ananya B. Sai and
                  Akash Kumar Mohankumar and
                  Mitesh M. Khapra},
  title        = {A Survey of Evaluation Metrics Used for {NLG} Systems},
  journal      = {CoRR},
  volume       = {abs/2008.12009},
  year         = {2020},
  url          = {https://arxiv.org/abs/2008.12009},
  eprinttype    = {arXiv},
  eprint       = {2008.12009},
  timestamp    = {Tue, 15 Sep 2020 20:52:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2008-12009.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Morrison2022,
  title = {Conventional histological and cytological staining with simultaneous immunohistochemistry enabled by invisible chromogens},
  volume = {102},
  ISSN = {0023-6837},
  url = {http://dx.doi.org/10.1038/s41374-021-00714-2},
  DOI = {10.1038/s41374-021-00714-2},
  number = {5},
  journal = {Laboratory Investigation},
  publisher = {Elsevier BV},
  author = {Morrison,  Larry E. and Lefever,  Mark R. and Lewis,  Heather N. and Kapadia,  Monesh J. and Bauer,  Daniel R.},
  year = {2022},
  month = may,
  pages = {545–553}
}

@article{Zarella2018,
    author = {Zarella, Mark D. and Bowman;, Douglas and Aeffner, Famke and Farahani, Navid and Xthona;, Albert and Absar, Syeda Fatima and Parwani, Anil and Bui, Marilyn and Hartman, Douglas J.},
    title = {A Practical Guide to Whole Slide Imaging: A White Paper From the Digital Pathology Association},
    journal = {Archives of Pathology \& Laboratory Medicine},
    volume = {143},
    number = {2},
    pages = {222-234},
    year = {2018},
    month = {10},
    abstract = {Whole slide imaging (WSI) represents a paradigm shift in pathology, serving as a necessary first step for a wide array of digital tools to enter the field. Its basic function is to digitize glass slides, but its impact on pathology workflows, reproducibility, dissemination of educational material, expansion of service to underprivileged areas, and intrainstitutional and interinstitutional collaboration exemplifies a significant innovative movement with far-reaching effects. Although the benefits of WSI to pathology practices, academic centers, and research institutions are many, the complexities of implementation remain an obstacle to widespread adoption. In the wake of the first regulatory clearance of WSI for primary diagnosis in the United States, some barriers to adoption have fallen. Nevertheless, implementation of WSI remains a difficult prospect for many institutions, especially those with stakeholders unfamiliar with the technologies necessary to implement a system or who cannot effectively communicate to executive leadership and sponsors the benefits of a technology that may lack clear and immediate reimbursement opportunity.To present an overview of WSI technology—present and future—and to demonstrate several immediate applications of WSI that support pathology practice, medical education, research, and collaboration.Peer-reviewed literature was reviewed by pathologists, scientists, and technologists who have practical knowledge of and experience with WSI.Implementation of WSI is a multifaceted and inherently multidisciplinary endeavor requiring contributions from pathologists, technologists, and executive leadership. Improved understanding of the current challenges to implementation, as well as the benefits and successes of the technology, can help prospective users identify the best path for success.},
    issn = {0003-9985},
    doi = {10.5858/arpa.2018-0343-RA},
    url = {https://doi.org/10.5858/arpa.2018-0343-RA},
    eprint = {https://meridian.allenpress.com/aplm/article-pdf/143/2/222/1448830/arpa\_2018-0343-ra.pdf},
}

@article{Dimitriou2019,
  title = {Deep Learning for Whole Slide Image Analysis: An Overview},
  volume = {6},
  ISSN = {2296-858X},
  url = {http://dx.doi.org/10.3389/fmed.2019.00264},
  DOI = {10.3389/fmed.2019.00264},
  journal = {Frontiers in Medicine},
  publisher = {Frontiers Media SA},
  author = {Dimitriou,  Neofytos and Arandjelović,  Ognjen and Caie,  Peter D.},
  year = {2019},
  month = nov 
}

@article{doi:10.1177/10668969231185089,
author = {Ekta Jain and Ankush Patel and Anil V. Parwani and Saba Shafi and Zoya Brar and Shivani Sharma and Sambit K. Mohanty},
title ={Whole Slide Imaging Technology and Its Applications: Current and Emerging Perspectives},
journal = {International Journal of Surgical Pathology},
volume = {32},
number = {3},
pages = {433-448},
year = {2024},
doi = {10.1177/10668969231185089},
note ={PMID: 37437093},
URL = {   
        https://doi.org/10.1177/10668969231185089
},
eprint = {     
        https://doi.org/10.1177/10668969231185089
}
,
    abstract = { Background. Whole slide imaging (WSI) represents a paradigm shift in pathology, serving as a necessary first step for a wide array of digital tools to enter the field. It utilizes virtual microscopy wherein glass slides are converted into digital slides and are viewed by pathologists by automated image analysis. Its impact on pathology workflow, reproducibility, dissemination of educational material, expansion of service to underprivileged areas, and institutional collaboration exemplifies a significant innovative movement. The recent US Food and Drug Administration approval to WSI for its use in primary surgical pathology diagnosis has opened opportunities for wider application of this technology in routine practice. Main Text. The ongoing technological advances in digital scanners, image visualization methods, and the integration of artificial intelligence-derived algorithms with these systems provide avenues to exploit its applications. Its benefits are innumerable such as ease of access through the internet, avoidance of physical storage space, and no risk of deterioration of staining quality or breakage of slides to name a few. Although the benefits of WSI to pathology practices are many, the complexities of implementation remain an obstacle to widespread adoption. Some barriers including the high cost, technical glitches, and most importantly professional hesitation to adopt a new technology have hindered its use in routine pathology. Conclusions. In this review, we summarize the technical aspects of WSI, its applications in diagnostic pathology, training, and research along with future perspectives. It also highlights improved understanding of the current challenges to implementation, as well as the benefits and successes of the technology. WSI provides a golden opportunity for pathologists to guide its evolution, standardization, and implementation to better acquaint them with the key aspects of this technology and its judicial use. Also, implementation of routine digital pathology is an extra step requiring resources which (currently) does not usually result increased efficiency or payment. }
}

@article{komura2018,
title = {Machine Learning Methods for Histopathological Image Analysis},
journal = {Computational and Structural Biotechnology Journal},
volume = {16},
pages = {34-42},
year = {2018},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2018.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2001037017300867},
author = {Daisuke Komura and Shumpei Ishikawa},
keywords = {Histopathology, Deep learning, Machine learning, Whole slide images, Computer assisted diagnosis, Digital image analysis},
abstract = {Abundant accumulation of digital histopathological images has led to the increased demand for their analysis, such as computer-aided diagnosis using machine learning techniques. However, digital pathological images and related tasks have some issues to be considered. In this mini-review, we introduce the application of digital pathological image analysis using machine learning algorithms, address some problems specific to such analysis, and propose possible solutions.}
}

@InProceedings{glorot2011,
  title = 	 {Deep Sparse Rectifier Neural Networks},
  author = 	 {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {315--323},
  year = 	 {2011},
  editor = 	 {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
  volume = 	 {15},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {11--13 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
  url = 	 {https://proceedings.mlr.press/v15/glorot11a.html},
  abstract = 	 {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.}
}

@article{krizhevsky2017,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet classification with deep convolutional neural networks},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3065386},
doi = {10.1145/3065386},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
journal = {Commun. ACM},
month = may,
pages = {84–90},
numpages = {7}
}

@Article{Lu2020,
author = {Lu, Lu and Yeonjong, Shin and Yanhui, Su and Karniadakis, George, Em},
title = {Dying {ReLU} and Initialization: Theory and Numerical Examples},
journal = {Communications in Computational Physics},
year = {2020},
volume = {28},
number = {5},
pages = {1671--1706},
abstract = {<p style="text-align: justify;">The dying ReLU refers to the problem when ReLU neurons become inactive
and only output 0 for any input. There are many empirical and heuristic explanations
of why ReLU neurons die. However, little is known about its theoretical analysis. In
this paper, we rigorously prove that a deep ReLU network will eventually die in probability as the depth goes to infinite. Several methods have been proposed to alleviate
the dying ReLU. Perhaps, one of the simplest treatments is to modify the initialization procedure. One common way of initializing weights and biases uses symmetric
probability distributions, which suffers from the dying ReLU. We thus propose a new
initialization procedure, namely, a randomized asymmetric initialization. We show
that the new initialization can effectively prevent the dying ReLU. All parameters required for the new initialization are theoretically designed. Numerical examples are
provided to demonstrate the effectiveness of the new initialization procedure.</p>},
issn = {1991-7120},
doi = {https://doi.org/10.4208/cicp.OA-2020-0165},
url = {https://global-sci.com/article/79737/dying-relu-and-initialization-theory-and-numerical-examples}
}

@misc{xu2015,
  doi = {10.48550/ARXIV.1505.00853},
  url = {https://arxiv.org/abs/1505.00853},
  author = {Xu,  Bing and Wang,  Naiyan and Chen,  Tianqi and Li,  Mu},
  keywords = {Machine Learning (cs.LG),  Computer Vision and Pattern Recognition (cs.CV),  Machine Learning (stat.ML),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Empirical Evaluation of Rectified Activations in Convolutional Network},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@article{Schmidt2019RecurrentNN,
  title={Recurrent Neural Networks {(RNNs)}: A gentle Introduction and Overview},
  author={Robin M. Schmidt},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.05911},
  url={https://api.semanticscholar.org/CorpusID:209324034}
}

@article{Coudray2018,
  title = {Classification and mutation prediction from non–small cell lung cancer histopathology images using deep learning},
  volume = {24},
  ISSN = {1546-170X},
  url = {http://dx.doi.org/10.1038/s41591-018-0177-5},
  DOI = {10.1038/s41591-018-0177-5},
  number = {10},
  journal = {Nature Medicine},
  publisher = {Springer Science and Business Media LLC},
  author = {Coudray,  Nicolas and Ocampo,  Paolo Santiago and Sakellaropoulos,  Theodore and Narula,  Navneet and Snuderl,  Matija and Feny\"{o},  David and Moreira,  Andre L. and Razavian,  Narges and Tsirigos,  Aristotelis},
  year = {2018},
  month = sep,
  pages = {1559–1567}
}

@article{ClaudioQuiros2024,
  title = {Mapping the landscape of histomorphological cancer phenotypes using self-supervised learning on unannotated pathology slides},
  volume = {15},
  ISSN = {2041-1723},
  url = {http://dx.doi.org/10.1038/s41467-024-48666-7},
  DOI = {10.1038/s41467-024-48666-7},
  number = {1},
  journal = {Nature Communications},
  publisher = {Springer Science and Business Media LLC},
  author = {Claudio Quiros,  Adalberto and Coudray,  Nicolas and Yeaton,  Anna and Yang,  Xinyu and Liu,  Bojing and Le,  Hortense and Chiriboga,  Luis and Karimkhan,  Afreen and Narula,  Navneet and Moore,  David A. and Park,  Christopher Y. and Pass,  Harvey and Moreira,  Andre L. and Le Quesne,  John and Tsirigos,  Aristotelis and Yuan,  Ke},
  year = {2024},
  month = jun 
}

@inproceedings{lundberg2017,
author = {Lundberg, Scott M. and Lee, Su-In},
title = {A unified approach to interpreting model predictions},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4768–4777},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@INPROCEEDINGS{resnet,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  keywords={Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
  doi={10.1109/CVPR.2016.90}}

@article{zbontar2021,
  title={Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  author={Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{\'e}phane},
  journal={arXiv preprint arXiv:2103.03230},
  year={2021}
}

@article{Traag2019,
  title = {From {Louvain} to {Leiden}: guaranteeing well-connected communities},
  volume = {9},
  ISSN = {2045-2322},
  url = {http://dx.doi.org/10.1038/s41598-019-41695-z},
  DOI = {10.1038/s41598-019-41695-z},
  number = {1},
  journal = {Scientific Reports},
  publisher = {Springer Science and Business Media LLC},
  author = {Traag,  V. A. and Waltman,  L. and van Eck,  N. J.},
  year = {2019},
  month = mar 
}

@article{Szymaski2025,
  title = {Fair Allocation Methods in Cooperative Games},
  url = {http://dx.doi.org/10.2139/ssrn.5074588},
  DOI = {10.2139/ssrn.5074588},
  publisher = {Elsevier BV},
  author = {Szymański,  Adam},
  year = {2025}
}

@INPROCEEDINGS{Vinyals2015,
  author={Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Show and tell: A neural image caption generator}, 
  year={2015},
  volume={},
  number={},
  pages={3156-3164},
  keywords={Logic gates;Measurement;Training;Visualization;Recurrent neural networks;Google},
  doi={10.1109/CVPR.2015.7298935}}

@inproceedings{mplug,
    title = "m{PLUG}: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections",
    author = "Li, Chenliang  and
      Xu, Haiyang  and
      Tian, Junfeng  and
      Wang, Wei  and
      Yan, Ming  and
      Bi, Bin  and
      Ye, Jiabo  and
      Chen, He  and
      Xu, Guohai  and
      Cao, Zheng  and
      Zhang, Ji  and
      Huang, Songfang  and
      Huang, Fei  and
      Zhou, Jingren  and
      Si, Luo",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.488/",
    doi = "10.18653/v1/2022.emnlp-main.488",
    pages = "7241--7259",
    abstract = "Large-scale pre-trained foundation models have been an emerging paradigm for building artificial intelligence (AI) systems, which can be quickly adapted to a wide range of downstream tasks. This paper presents mPLUG, a new vision-language foundation model for both cross-modal understanding and generation. Most existing pre-trained models suffer from inefficiency and linguistic signal overwhelmed by long visual sequences in cross-modal alignment. To address both problems, mPLUG introduces an effective and efficient vision-language architecture with novel cross-modal skip-connections.mPLUG is pre-trained end-to-end on large-scale image-text pairs with both discriminative and generative objectives. It achieves state-of-the-art results on a wide range of vision-language downstream tasks, including image captioning, image-text retrieval, visual grounding and visual question answering. mPLUG also demonstrates strong zero-shot transferability on vision-language and video-language tasks. The code and pre-trained models are available at https://github.com/alibaba/AliceMind"
}

@inproceedings{blip2,
author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
title = {{BLIP-2}: bootstrapping language-image pre-training with frozen image encoders and large language models},
year = {2023},
publisher = {JMLR.org},
abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pretraining strategy that bootstraps vision-language pre-training from off-the-shelf frozen pretrained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pretrained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {814},
numpages = {13},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@misc{GIT,
      title={{GIT}: A Generative Image-to-text Transformer for Vision and Language}, 
      author={Jianfeng Wang and Zhengyuan Yang and Xiaowei Hu and Linjie Li and Kevin Lin and Zhe Gan and Zicheng Liu and Ce Liu and Lijuan Wang},
      year={2022},
      eprint={2205.14100},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2205.14100}, 
}

@article{srivastava2014,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
title = {Dropout: a simple way to prevent neural networks from overfitting},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1929–1958},
numpages = {30},
keywords = {deep learning, model combination, neural networks, regularization}
}

@inproceedings{lin2004,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013/",
    pages = "74--81"
}

@inproceedings{papineni2002,
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
title = {{BLEU}: a method for automatic evaluation of machine translation},
year = {2002},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1073083.1073135},
doi = {10.3115/1073083.1073135},
abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
pages = {311–318},
numpages = {8},
location = {Philadelphia, Pennsylvania},
series = {ACL '02}
}

@misc{ShapDoc,
  title = {An introduction to explainable {AI} with {Shapley} values},
  howpublished = {\url{https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html}},
  note = {Last accessed: 2025-02-15},
  year = {2018},
  author = {Lundberg, Scott}}

@misc{kingma2017,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}

@article{lstm1997,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = nov,
pages = {1735–1780},
numpages = {46}
}

@ARTICLE{bptt,
  author={Werbos, P.J.},
  journal={Proceedings of the IEEE}, 
  title={Backpropagation through time: what it does and how to do it}, 
  year={1990},
  volume={78},
  number={10},
  pages={1550-1560},
  keywords={Backpropagation;Artificial neural networks;Supervised learning;Pattern recognition;Neural networks;Power system modeling;Equations;Control systems;Fluid dynamics;Books},
  doi={10.1109/5.58337}}


@Inbook{Artstein2017,
author="Artstein, Ron",
title="Inter-annotator Agreement",
bookTitle="Handbook of Linguistic Annotation",
year="2017",
publisher="Springer Netherlands",
address="Dordrecht",
pages="297--313",
abstract="This chapter touches upon several issues in the calculation and assessment of inter-annotator agreement. It gives an introduction to the theory behind agreement coefficients and examples of their application to linguistic annotation tasks. Specific examples explore variation in annotator performance due to heterogeneous data, complex labels, item difficulty, and annotator differences, showing how global agreement coefficients may mask these sources of variation, and how detailed agreement studies can give insight into both the annotation process and the nature of the underlying data. The chapter also reviews recent work on using machine learning to exploit the variation among annotators and learn detailed models from which accurate labels can be inferred. I therefore advocate an approach where agreement studies are not used merely as a means to accept or reject a particular annotation scheme, but as a tool for exploring patterns in the data that are being annotated.",
isbn="978-94-024-0881-2",
doi={10.1007/978-94-024-0881-2_11},
url={https://doi.org/10.1007/978-94-024-0881-2\_11}
}


@InProceedings{cohen2019,
  title = 	 {Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models},
  author =       {Cohen, Eldan and Beck, Christopher},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1290--1299},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/cohen19a/cohen19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/cohen19a.html},
  abstract = 	 {Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, work on a number of applications has found that the quality of the highest probability hypothesis found by beam search degrades with large beam widths. We perform an empirical study of the behavior of beam search across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early, very low probability tokens that are followed by a sequence of tokens with higher (conditional) probability. We show that, empirically, such sequences are more likely to have a lower evaluation score than lower probability sequences without this pattern. Using the notion of search discrepancies from heuristic search, we hypothesize that large discrepancies are the cause of the performance degradation. We show that this hypothesis generalizes the previous ones in machine translation and image captioning. To validate our hypothesis, we show that constraining beam search to avoid large discrepancies eliminates the performance degradation.}
}

@Article{kumar2022,
AUTHOR = {Kumar, Deepika and Srivastava, Varun and Popescu, Daniela Elena and Hemanth, Jude D.},
TITLE = {Dual-Modal Transformer with Enhanced Inter- and Intra-Modality Interactions for Image Captioning},
JOURNAL = {Applied Sciences},
VOLUME = {12},
YEAR = {2022},
NUMBER = {13},
ARTICLE-NUMBER = {6733},
URL = {https://www.mdpi.com/2076-3417/12/13/6733},
ISSN = {2076-3417},
ABSTRACT = {Image captioning is oriented towards describing an image with the best possible use of words that can provide a semantic, relatable meaning of the scenario inscribed. Different models can be used to accomplish this arduous task depending on the context and requirement of what needs to be achieved. An encoder–decoder model which uses the image feature vectors as an input to the encoder is often marked as one of the appropriate models to accomplish the captioning process. In the proposed work, a dual-modal transformer has been used which captures the intra- and inter-model interactions in a simultaneous manner within an attention block. The transformer architecture is quantitatively evaluated on a publicly available Microsoft Common Objects in Context (MS COCO) dataset yielding a Bilingual Evaluation Understudy (BLEU)-4 Score of 85.01. The efficacy of the model is evaluated on Flickr 8k, Flickr 30k datasets and MS COCO datasets and results for the same is compared and analysed with the state-of-the-art methods. The results shows that the proposed model outperformed when compared with conventional models, such as the encoder–decoder model and attention model.},
DOI = {10.3390/app12136733}
}

@misc{vaswani2023,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{cptac-cm,
  doi = {10.7937/K9/TCIA.2018.ODU24GZE},
  url = {https://www.cancerimagingarchive.net/collection/cptac-cm/},
  author = {{National Cancer Institute Clinical Proteomic Tumor Analysis Consortium (CPTAC)}},
  title = {The Clinical Proteomic Tumor Analysis Consortium Cutaneous Melanoma Collection (CPTAC-CM)},
  publisher = {The Cancer Imaging Archive},
  year = {2018},
  copyright = {Creative Commons Attribution 3.0 Unported}
}

@misc{cmb-pca,
  doi = {10.7937/25T7-6Y12},
  url = {https://www.cancerimagingarchive.net/collection/cmb-pca/},
  author = {{Cancer Moonshot Biobank}},
  title = {Cancer Moonshot Biobank - Prostate Cancer Collection (CMB-PCA)},
  publisher = {The Cancer Imaging Archive},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{cmb-brca,
  doi = {10.7937/DX22-8J71},
  url = {https://www.cancerimagingarchive.net/collection/cmb-brca/},
  author = {{Cancer Moonshot Biobank}},
  title = {Cancer Moonshot Biobank - Invasive Breast Carcinoma Cancer Collection (CMB-BRCA)},
  publisher = {The Cancer Imaging Archive},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{cptac-luad,
  doi = {10.7937/K9/TCIA.2018.PAT12TBS},
  url = {https://www.cancerimagingarchive.net/collection/cptac-luad/},
  author = {{National Cancer Institute Clinical Proteomic Tumor Analysis Consortium (CPTAC)}},
  title = {The Clinical Proteomic Tumor Analysis Consortium Lung Adenocarcinoma Collection (CPTAC-LUAD)},
  publisher = {The Cancer Imaging Archive},
  year = {2018},
  copyright = {Creative Commons Attribution 3.0 Unported}
}

@misc{cptac-lscc,
  doi = {10.7937/K9/TCIA.2018.6EMUB5L2},
  url = {https://www.cancerimagingarchive.net/collection/cptac-lscc/},
  author = {{National Cancer Institute Clinical Proteomic Tumor Analysis Consortium (CPTAC)}},
  title = {The Clinical Proteomic Tumor Analysis Consortium Lung Squamous Cell Carcinoma Collection (CPTAC-LSCC)},
  publisher = {The Cancer Imaging Archive},
  year = {2018},
  copyright = {Creative Commons Attribution 3.0 Unported}
}

@misc{cptac-ucec,
  doi = {10.7937/K9/TCIA.2018.3R3JUISW},
  url = {https://www.cancerimagingarchive.net/collection/cptac-ucec/},
  author = {{National Cancer Institute Clinical Proteomic Tumor Analysis Consortium (CPTAC)}},
  title = {The Clinical Proteomic Tumor Analysis Consortium Uterine Corpus Endometrial Carcinoma Collection (CPTAC-UCEC)},
  publisher = {The Cancer Imaging Archive},
  year = {2019},
  copyright = {Creative Commons Attribution 3.0 Unported}
}

@misc{cptac-coad,
  doi = {10.7937/TCIA.YZWQ-ZZ63},
  url = {https://www.cancerimagingarchive.net/collection/cptac-coad/},
  author = {{National Cancer Institute Clinical Proteomic Tumor Analysis Consortium (CPTAC)}},
  title = {The Clinical Proteomic Tumor Analysis Consortium Colon Adenocarcinoma Collection (CPTAC-COAD)},
  publisher = {The Cancer Imaging Archive},
  year = {2020},
  copyright = {Creative Commons Attribution 3.0 Unported}
}

@misc{opitz2021,
      title={Macro F1 and Macro F1}, 
      author={Juri Opitz and Sebastian Burst},
      year={2021},
      eprint={1911.03347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1911.03347}, 
}